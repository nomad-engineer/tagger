"""
SQLite database schema and management for Image Tagger

The database is a REBUILDABLE cache/index built from JSON files.
If the database is corrupted or missing, it can be regenerated by scanning
the images/ directory and reading all .json files.
"""

import sqlite3
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import json


class Database:
    """
    SQLite database wrapper for image library metadata

    This database is a performance optimization - all data is stored in
    human-readable JSON files and can be rebuilt from those files.
    """

    def __init__(self, db_path: Path):
        """
        Initialize database connection

        Args:
            db_path: Path to the SQLite database file (usually library.db)
        """
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None

    def connect(self):
        """Open connection to database"""
        self.conn = sqlite3.connect(str(self.db_path))
        self.conn.row_factory = sqlite3.Row  # Return rows as dict-like objects
        # Enable foreign keys
        self.conn.execute("PRAGMA foreign_keys = ON")

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def __enter__(self):
        """Context manager entry"""
        self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.close()

    def create_schema(self):
        """
        Create database schema
        """
        if not self.conn:
            raise RuntimeError("Database not connected")

        cursor = self.conn.cursor()

        # Media table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS media (
                hash TEXT PRIMARY KEY,
                type TEXT NOT NULL CHECK(type IN ('image', 'video', 'mask', 'video_frame', 'crop')),
                source_media TEXT,
                name TEXT,
                caption TEXT,
                created DATETIME,
                modified DATETIME,
                metadata_json TEXT
            )
        """)

        cursor.execute("CREATE INDEX IF NOT EXISTS idx_media_type ON media(type)")
        cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_media_source ON media(source_media)"
        )

        # Tags table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS tags (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                media_hash TEXT NOT NULL,
                category TEXT NOT NULL,
                value TEXT NOT NULL,
                position INTEGER NOT NULL,
                FOREIGN KEY (media_hash) REFERENCES media(hash) ON DELETE CASCADE
            )
        """)

        cursor.execute("CREATE INDEX IF NOT EXISTS idx_tags_media ON tags(media_hash)")
        cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_tags_cat_val ON tags(category, value)"
        )

        # FTS5 for tags
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS tags_fts USING fts5(
                category,
                value,
                media_hash UNINDEXED,
                content=tags,
                content_rowid=id
            )
        """)

        # Triggers
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS tags_fts_insert AFTER INSERT ON tags BEGIN
                INSERT INTO tags_fts(rowid, category, value, media_hash)
                VALUES (new.id, new.category, new.value, new.media_hash);
            END
        """)
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS tags_fts_delete AFTER DELETE ON tags BEGIN
                INSERT INTO tags_fts(tags_fts, rowid, category, value, media_hash)
                VALUES('delete', old.id, old.category, old.value, old.media_hash);
            END
        """)
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS tags_fts_update AFTER UPDATE ON tags BEGIN
                INSERT INTO tags_fts(tags_fts, rowid, category, value, media_hash)
                VALUES('delete', old.id, old.category, old.value, old.media_hash);
                INSERT INTO tags_fts(rowid, category, value, media_hash)
                VALUES (new.id, new.category, new.value, new.media_hash);
            END
        """)

        # Relationships
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS relationships (
                from_hash TEXT NOT NULL,
                to_hash TEXT NOT NULL,
                type TEXT NOT NULL,
                strength REAL,
                metadata_json TEXT,
                PRIMARY KEY (from_hash, to_hash, type),
                FOREIGN KEY (from_hash) REFERENCES media(hash) ON DELETE CASCADE,
                FOREIGN KEY (to_hash) REFERENCES media(hash) ON DELETE CASCADE
            )
        """)

        # Perceptual hashes
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS perceptual_hashes (
                media_hash TEXT NOT NULL,
                algorithm TEXT NOT NULL,
                hash_value TEXT NOT NULL,
                computed DATETIME,
                PRIMARY KEY (media_hash, algorithm),
                FOREIGN KEY (media_hash) REFERENCES media(hash) ON DELETE CASCADE
            )
        """)

        # Metadata
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS library_metadata (
                key TEXT PRIMARY KEY,
                value_json TEXT,
                modified DATETIME
            )
        """)

        self.conn.commit()

    def check_and_migrate_schema(self):
        """
        Check schema version and migrate if necessary
        """
        if not self.conn:
            raise RuntimeError("Database not connected")

        current_version = self.get_schema_version()

        # Target version is 3 (support for 'video' type and fixed foreign keys)
        TARGET_VERSION = 3

        if current_version < TARGET_VERSION:
            print(
                f"Migrating database schema from v{current_version} to v{TARGET_VERSION}..."
            )

            try:
                # Disable foreign keys during migration to allow table recreation
                self.conn.execute("PRAGMA foreign_keys = OFF")

                # Since this is a rebuildable cache, for major schema changes (like FK fixes)
                # we just drop and recreate everything. This ensures a clean state and correct FKs.
                self.drop_all_tables()
                self.create_schema()

                # Update schema version
                self.set_schema_version(TARGET_VERSION)

                # Re-enable foreign keys
                self.conn.execute("PRAGMA foreign_keys = ON")

                print(
                    f"Schema migration to v{TARGET_VERSION} complete. Database is ready for rebuild."
                )

            except Exception as e:
                print(f"Error migrating database schema: {e}")
                if self.conn:
                    self.conn.rollback()
                    self.conn.execute("PRAGMA foreign_keys = ON")
                raise

    def _recreate_all_tables_with_new_media(self):
        """
        Robust migration: Recreate all tables to ensure foreign keys point to the new media table.
        This handles additions of types like 'crop' and 'video'.
        """
        cursor = self.conn.cursor()

        print("Recreating all tables to update constraints and foreign keys...")

        # 1. Rename existing tables
        cursor.execute("ALTER TABLE media RENAME TO media_old")
        cursor.execute("ALTER TABLE tags RENAME TO tags_old")
        cursor.execute("ALTER TABLE relationships RENAME TO relationships_old")

        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='perceptual_hashes'"
        )
        has_phash = cursor.fetchone() is not None
        if has_phash:
            cursor.execute("ALTER TABLE perceptual_hashes RENAME TO ph_old")

        # 2. Create new schema using the standard method
        self.create_schema()

        # 3. Copy data for media (handling potential invalid types by letting them fail or filtering)
        # We use a sub-select to only copy valid types if we wanted to be super safe,
        # but here we trust the migration logic.
        cursor.execute("""
            INSERT OR IGNORE INTO media (hash, type, source_media, name, caption, created, modified, metadata_json)
            SELECT hash, type, source_media, name, caption, created, modified, metadata_json
            FROM media_old
        """)

        # 4. Copy data for tags
        cursor.execute("""
            INSERT OR IGNORE INTO tags (id, media_hash, category, value, position)
            SELECT id, media_hash, category, value, position FROM tags_old
        """)

        # 5. Copy data for relationships
        cursor.execute("""
            INSERT OR IGNORE INTO relationships (from_hash, to_hash, type, strength, metadata_json)
            SELECT from_hash, to_hash, type, strength, metadata_json FROM relationships_old
        """)

        # 6. Copy data for perceptual hashes
        if has_phash:
            cursor.execute("""
                INSERT OR IGNORE INTO perceptual_hashes (media_hash, algorithm, hash_value, computed)
                SELECT media_hash, algorithm, hash_value, computed FROM ph_old
            """)

        # 7. Drop old tables
        cursor.execute("DROP TABLE media_old")
        cursor.execute("DROP TABLE tags_old")
        cursor.execute("DROP TABLE relationships_old")
        if has_phash:
            cursor.execute("DROP TABLE ph_old")

        self.conn.commit()

    def drop_all_tables(self):
        """
        Drop all tables - useful for complete rebuild

        WARNING: This destroys the entire database. Only use when rebuilding
        from JSON files.
        """
        if not self.conn:
            raise RuntimeError("Database not connected")

        cursor = self.conn.cursor()

        # Drop triggers first
        cursor.execute("DROP TRIGGER IF EXISTS tags_fts_insert")
        cursor.execute("DROP TRIGGER IF EXISTS tags_fts_delete")
        cursor.execute("DROP TRIGGER IF EXISTS tags_fts_update")

        # Drop tables
        cursor.execute("DROP TABLE IF EXISTS tags_fts")
        cursor.execute("DROP TABLE IF EXISTS perceptual_hashes")
        cursor.execute("DROP TABLE IF EXISTS relationships")
        cursor.execute("DROP TABLE IF EXISTS tags")
        cursor.execute("DROP TABLE IF EXISTS media")
        cursor.execute("DROP TABLE IF EXISTS library_metadata")

        self.conn.commit()

    def get_schema_version(self) -> int:
        """Get current schema version (for future migrations)"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        try:
            cursor = self.conn.execute(
                "SELECT value_json FROM library_metadata WHERE key = 'schema_version'"
            )
            row = cursor.fetchone()
            if row:
                return int(json.loads(row[0]))
        except sqlite3.OperationalError:
            # Table doesn't exist yet
            pass

        return 0

    def set_schema_version(self, version: int):
        """Set schema version"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        now = datetime.now().isoformat()
        self.conn.execute(
            """
            INSERT OR REPLACE INTO library_metadata (key, value_json, modified)
            VALUES ('schema_version', ?, ?)
            """,
            (json.dumps(version), now),
        )
        self.conn.commit()

    def vacuum(self):
        """Optimize database (reclaim space, rebuild indexes)"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        self.conn.execute("VACUUM")
        self.conn.execute("ANALYZE")

    def get_stats(self) -> Dict[str, int]:
        """Get database statistics"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        stats = {}

        # Count media by type
        cursor = self.conn.execute("""
            SELECT type, COUNT(*) as count
            FROM media
            GROUP BY type
        """)
        for row in cursor:
            stats[f"{row['type']}_count"] = row["count"]

        # Total tags
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM tags")
        stats["tag_count"] = cursor.fetchone()["count"]

        # Total relationships
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM relationships")
        stats["relationship_count"] = cursor.fetchone()["count"]

        # Perceptual hashes
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM perceptual_hashes")
        stats["phash_count"] = cursor.fetchone()["count"]

        return stats


def create_database(db_path: Path) -> Database:
    """
    Create a new database with schema

    Args:
        db_path: Path where database should be created

    Returns:
        Database instance with schema initialized
    """
    # Remove existing database if it exists
    if db_path.exists():
        db_path.unlink()

    db = Database(db_path)
    db.connect()
    db.create_schema()
    db.set_schema_version(2)  # Set to current version

    return db


def rebuild_database(db_path: Path, images_dir: Path) -> Database:
    """
    Rebuild database from JSON files in images directory

    This scans all .json files in the images/ directory and populates
    the database. Useful for:
    - Initial database creation
    - Recovery from corruption
    - Migrating from old format

    Args:
        db_path: Path to database file
        images_dir: Path to images/ directory containing .json files

    Returns:
        Database instance with data loaded
    """
    db = create_database(db_path)

    # Import here to avoid circular dependency
    from .data_models import MediaData

    # Scan all .json files in images directory
    json_files = list(images_dir.glob("*.json"))

    for json_path in json_files:
        try:
            # Load media data
            with open(json_path, "r") as f:
                data = json.load(f)

            # Get media hash from filename
            media_hash = json_path.stem

            # Create appropriate media object
            media = MediaData.from_dict(data)

            # Insert into database (would use DatabaseRepository methods here)
            # For now, just validate the file is readable
            # The actual insertion will be done by DatabaseRepository

        except Exception as e:
            print(f"Warning: Could not read {json_path}: {e}")
            continue

    return db
