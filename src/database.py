"""
SQLite database schema and management for Image Tagger

The database is a REBUILDABLE cache/index built from JSON files.
If the database is corrupted or missing, it can be regenerated by scanning
the images/ directory and reading all .json files.
"""

import sqlite3
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import json


class Database:
    """
    SQLite database wrapper for image library metadata

    This database is a performance optimization - all data is stored in
    human-readable JSON files and can be rebuilt from those files.
    """

    def __init__(self, db_path: Path):
        """
        Initialize database connection

        Args:
            db_path: Path to the SQLite database file (usually library.db)
        """
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None

    def connect(self):
        """Open connection to database"""
        self.conn = sqlite3.connect(str(self.db_path))
        self.conn.row_factory = sqlite3.Row  # Return rows as dict-like objects
        # Enable foreign keys
        self.conn.execute("PRAGMA foreign_keys = ON")

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def __enter__(self):
        """Context manager entry"""
        self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.close()

    def create_schema(self):
        """
        Create database schema

        This initializes all tables and indexes needed for the application.
        Safe to call multiple times (uses IF NOT EXISTS).
        """
        if not self.conn:
            raise RuntimeError("Database not connected")

        cursor = self.conn.cursor()

        # Media table - stores all media items (images, masks, video frames, crops)
        # Note: Updated to include 'crop' type
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS media (
                hash TEXT PRIMARY KEY,
                type TEXT NOT NULL CHECK(type IN ('image', 'mask', 'video_frame', 'crop')),
                source_media TEXT,  -- For masks, video frames, crops: hash of source image/video
                name TEXT,
                caption TEXT,
                created DATETIME,
                modified DATETIME,
                metadata_json TEXT  -- JSON blob for additional metadata
            )
        """)

        # Create index on type for filtering
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_media_type ON media(type)
        """)

        # Create index on source_media for finding all masks/frames of a source
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_media_source ON media(source_media)
        """)

        # Tags table - normalized tag storage
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS tags (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                media_hash TEXT NOT NULL,
                category TEXT NOT NULL,
                value TEXT NOT NULL,
                position INTEGER NOT NULL,  -- Order/importance (0 = most important)
                FOREIGN KEY (media_hash) REFERENCES media(hash) ON DELETE CASCADE
            )
        """)

        # Create indexes for tag queries
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_tags_media ON tags(media_hash)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_tags_category ON tags(category)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_tags_value ON tags(value)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_tags_cat_val ON tags(category, value)
        """)

        # FTS5 virtual table for full-text search on tags
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS tags_fts USING fts5(
                category,
                value,
                media_hash UNINDEXED,
                content=tags,
                content_rowid=id
            )
        """)

        # Triggers to keep FTS5 table in sync with tags table
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS tags_fts_insert AFTER INSERT ON tags BEGIN
                INSERT INTO tags_fts(rowid, category, value, media_hash)
                VALUES (new.id, new.category, new.value, new.media_hash);
            END
        """)

        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS tags_fts_delete AFTER DELETE ON tags BEGIN
                INSERT INTO tags_fts(tags_fts, rowid, category, value, media_hash)
                VALUES('delete', old.id, old.category, old.value, old.media_hash);
            END
        """)

        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS tags_fts_update AFTER UPDATE ON tags BEGIN
                INSERT INTO tags_fts(tags_fts, rowid, category, value, media_hash)
                VALUES('delete', old.id, old.category, old.value, old.media_hash);
                INSERT INTO tags_fts(rowid, category, value, media_hash)
                VALUES (new.id, new.category, new.value, new.media_hash);
            END
        """)

        # Relationships table - for similar images, masks, video sequences, etc.
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS relationships (
                from_hash TEXT NOT NULL,
                to_hash TEXT NOT NULL,
                type TEXT NOT NULL,  -- 'similar', 'mask_of', 'video_sequence'
                strength REAL,  -- Similarity score (0-1) or NULL for non-similarity relationships
                metadata_json TEXT,  -- Additional relationship metadata as JSON
                PRIMARY KEY (from_hash, to_hash, type),
                FOREIGN KEY (from_hash) REFERENCES media(hash) ON DELETE CASCADE,
                FOREIGN KEY (to_hash) REFERENCES media(hash) ON DELETE CASCADE
            )
        """)

        # Create indexes for relationship queries
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_rel_from ON relationships(from_hash, type)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_rel_to ON relationships(to_hash, type)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_rel_type ON relationships(type)
        """)

        # Perceptual hashes table - cached perceptual hashes for similarity detection
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS perceptual_hashes (
                media_hash TEXT NOT NULL,
                algorithm TEXT NOT NULL,  -- 'phash', 'dhash', 'ahash', 'whash'
                hash_value TEXT NOT NULL,
                computed DATETIME,
                PRIMARY KEY (media_hash, algorithm),
                FOREIGN KEY (media_hash) REFERENCES media(hash) ON DELETE CASCADE
            )
        """)

        # Create index for finding similar images by hash value
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_phash_value ON perceptual_hashes(algorithm, hash_value)
        """)

        # Metadata table - library-level metadata
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS library_metadata (
                key TEXT PRIMARY KEY,
                value_json TEXT,
                modified DATETIME
            )
        """)

        self.conn.commit()

    def check_and_migrate_schema(self):
        """
        Check schema version and migrate if necessary
        """
        if not self.conn:
            raise RuntimeError("Database not connected")

        current_version = self.get_schema_version()

        # Target version is 2 (support for 'crop' type)
        TARGET_VERSION = 2

        if current_version < TARGET_VERSION:
            print(
                f"Migrating database schema from v{current_version} to v{TARGET_VERSION}..."
            )

            try:
                # Migrate to v2: Add 'crop' to media type check constraint
                if current_version < 2:
                    self._migrate_to_v2()

                # Update schema version
                self.set_schema_version(TARGET_VERSION)
                print(f"Schema migration to v{TARGET_VERSION} complete.")

            except Exception as e:
                print(f"Error migrating database schema: {e}")
                self.conn.rollback()
                raise

    def _migrate_to_v2(self):
        """Migrate to v2: Add 'crop' to allowed media types"""
        cursor = self.conn.cursor()

        # SQLite doesn't support altering CHECK constraints directly.
        # We must recreate the table.

        print("Recreating media table to update CHECK constraints...")

        # 1. Rename existing table
        cursor.execute("ALTER TABLE media RENAME TO media_old")

        # 2. Create new table with updated constraint
        cursor.execute("""
            CREATE TABLE media (
                hash TEXT PRIMARY KEY,
                type TEXT NOT NULL CHECK(type IN ('image', 'mask', 'video_frame', 'crop')),
                source_media TEXT,
                name TEXT,
                caption TEXT,
                created DATETIME,
                modified DATETIME,
                metadata_json TEXT
            )
        """)

        # 3. Copy data
        cursor.execute("""
            INSERT INTO media (hash, type, source_media, name, caption, created, modified, metadata_json)
            SELECT hash, type, source_media, name, caption, created, modified, metadata_json
            FROM media_old
        """)

        # 4. Drop old table
        cursor.execute("DROP TABLE media_old")

        # 5. Recreate indexes associated with media table
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_media_type ON media(type)")
        cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_media_source ON media(source_media)"
        )

        self.conn.commit()

    def drop_all_tables(self):
        """
        Drop all tables - useful for complete rebuild

        WARNING: This destroys the entire database. Only use when rebuilding
        from JSON files.
        """
        if not self.conn:
            raise RuntimeError("Database not connected")

        cursor = self.conn.cursor()

        # Drop triggers first
        cursor.execute("DROP TRIGGER IF EXISTS tags_fts_insert")
        cursor.execute("DROP TRIGGER IF EXISTS tags_fts_delete")
        cursor.execute("DROP TRIGGER IF EXISTS tags_fts_update")

        # Drop tables
        cursor.execute("DROP TABLE IF EXISTS tags_fts")
        cursor.execute("DROP TABLE IF EXISTS perceptual_hashes")
        cursor.execute("DROP TABLE IF EXISTS relationships")
        cursor.execute("DROP TABLE IF EXISTS tags")
        cursor.execute("DROP TABLE IF EXISTS media")
        cursor.execute("DROP TABLE IF EXISTS library_metadata")

        self.conn.commit()

    def get_schema_version(self) -> int:
        """Get current schema version (for future migrations)"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        try:
            cursor = self.conn.execute(
                "SELECT value_json FROM library_metadata WHERE key = 'schema_version'"
            )
            row = cursor.fetchone()
            if row:
                return int(json.loads(row[0]))
        except sqlite3.OperationalError:
            # Table doesn't exist yet
            pass

        return 0

    def set_schema_version(self, version: int):
        """Set schema version"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        now = datetime.now().isoformat()
        self.conn.execute(
            """
            INSERT OR REPLACE INTO library_metadata (key, value_json, modified)
            VALUES ('schema_version', ?, ?)
            """,
            (json.dumps(version), now),
        )
        self.conn.commit()

    def vacuum(self):
        """Optimize database (reclaim space, rebuild indexes)"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        self.conn.execute("VACUUM")
        self.conn.execute("ANALYZE")

    def get_stats(self) -> Dict[str, int]:
        """Get database statistics"""
        if not self.conn:
            raise RuntimeError("Database not connected")

        stats = {}

        # Count media by type
        cursor = self.conn.execute("""
            SELECT type, COUNT(*) as count
            FROM media
            GROUP BY type
        """)
        for row in cursor:
            stats[f"{row['type']}_count"] = row["count"]

        # Total tags
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM tags")
        stats["tag_count"] = cursor.fetchone()["count"]

        # Total relationships
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM relationships")
        stats["relationship_count"] = cursor.fetchone()["count"]

        # Perceptual hashes
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM perceptual_hashes")
        stats["phash_count"] = cursor.fetchone()["count"]

        return stats


def create_database(db_path: Path) -> Database:
    """
    Create a new database with schema

    Args:
        db_path: Path where database should be created

    Returns:
        Database instance with schema initialized
    """
    # Remove existing database if it exists
    if db_path.exists():
        db_path.unlink()

    db = Database(db_path)
    db.connect()
    db.create_schema()
    db.set_schema_version(2)  # Set to current version

    return db


def rebuild_database(db_path: Path, images_dir: Path) -> Database:
    """
    Rebuild database from JSON files in images directory

    This scans all .json files in the images/ directory and populates
    the database. Useful for:
    - Initial database creation
    - Recovery from corruption
    - Migrating from old format

    Args:
        db_path: Path to database file
        images_dir: Path to images/ directory containing .json files

    Returns:
        Database instance with data loaded
    """
    db = create_database(db_path)

    # Import here to avoid circular dependency
    from .data_models import MediaData

    # Scan all .json files in images directory
    json_files = list(images_dir.glob("*.json"))

    for json_path in json_files:
        try:
            # Load media data
            with open(json_path, "r") as f:
                data = json.load(f)

            # Get media hash from filename
            media_hash = json_path.stem

            # Create appropriate media object
            media = MediaData.from_dict(data)

            # Insert into database (would use DatabaseRepository methods here)
            # For now, just validate the file is readable
            # The actual insertion will be done by DatabaseRepository

        except Exception as e:
            print(f"Warning: Could not read {json_path}: {e}")
            continue

    return db
